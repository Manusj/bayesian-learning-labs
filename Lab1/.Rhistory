#I have seen the same thing for Jos?'s code. If I run the full code it doesn't show. However, if I only run the last part I get that error (along with a warning that  the algorithm (training of nn) does not converge)
#[11:47] Namita Sharma
#Filip Ekstr?m Yes, I was able to get the error the same way you described.
#[11:54] Jose M Pena
#Hi, I also get the error if I do as Filip. The error does not seem to be caused during training (by the function neuralnet) but by testing (by the function predict). If you just run neuralnet, then you just get the warning of no convergence but no error, which according to the manual leads to stopping the training process. It may be that the resulting NN is not meant to be used for prediction when training is stopped this way. So, I think that Namita's suggestion of setting the threshold to, for instance, 0.1 will do the job. Anyway, the goal of this exercise is to show that NNs fail (so raising the threshold will not change the goal of the exercise).
ht2 <- function(x) ifelse(x>0,0, x)
winit <- runif(31, -1, 1)
nn <- neuralnet(formula = Sin ~ Var, data = tr, hidden = 10, startweights = winit, act.fct = ht2)
plot(tr, cex=2)
points(te, col = "blue", cex=1)
points(te[,1],predict(nn,te), col="red", cex=1)
ht2 <- function(x) ifelse(x>0,0, x)
winit <- runif(31, -1, 1)
nn <- neuralnet(formula = Sin ~ Var, data = tr, hidden = 10, startweights = winit, act.fct = ht2)
plot(tr, cex=2)
points(te, col = "blue", cex=1)
points(te[,1],predict(nn,te), col="red", cex=1)
ht2 <- function(x) ifelse(x>0,x, 0)
winit <- runif(31, -1, 1)
nn <- neuralnet(formula = Sin ~ Var, data = tr, hidden = 10, startweights = winit, act.fct = ht2)
plot(tr, cex=2)
points(te, col = "blue", cex=1)
points(te[,1],predict(nn,te), col="red", cex=1)
# Solutions to the lab 3 block 1 of 732A99/TDDE01/732A68 Machine Learning
# Author: jose.m.pena@liu.se
# Made for teaching purposes
library(kernlab)
set.seed(1234567890)
data(spam)
foo <- sample(nrow(spam))
spam <- spam[foo,]
spam[,-58]<-scale(spam[,-58])
tr <- spam[1:3000, ]
va <- spam[3001:3800, ]
trva <- spam[1:3800, ]
te <- spam[3801:4601, ]
by <- 0.3
err_va <- NULL
for(i in seq(by,5,by)){
filter <- ksvm(type~.,data=tr,kernel="rbfdot",kpar=list(sigma=0.05),C=i,scaled=FALSE)
mailtype <- predict(filter,va[,-58])
t <- table(mailtype,va[,58])
err_va <-c(err_va,(t[1,2]+t[2,1])/sum(t))
}
filter0 <- ksvm(type~.,data=tr,kernel="rbfdot",kpar=list(sigma=0.05),C=which.min(err_va)*by,scaled=FALSE)
mailtype <- predict(filter0,va[,-58])
t <- table(mailtype,va[,58])
err0 <- (t[1,2]+t[2,1])/sum(t)
err0
filter1 <- ksvm(type~.,data=tr,kernel="rbfdot",kpar=list(sigma=0.05),C=which.min(err_va)*by,scaled=FALSE)
mailtype <- predict(filter1,te[,-58])
t <- table(mailtype,te[,58])
err1 <- (t[1,2]+t[2,1])/sum(t)
err1
filter2 <- ksvm(type~.,data=trva,kernel="rbfdot",kpar=list(sigma=0.05),C=which.min(err_va)*by,scaled=FALSE)
mailtype <- predict(filter2,te[,-58])
t <- table(mailtype,te[,58])
err2 <- (t[1,2]+t[2,1])/sum(t)
err2
filter3 <- ksvm(type~.,data=spam,kernel="rbfdot",kpar=list(sigma=0.05),C=which.min(err_va)*by,scaled=FALSE)
mailtype <- predict(filter3,te[,-58])
t <- table(mailtype,te[,58])
err3 <- (t[1,2]+t[2,1])/sum(t)
err3
# Questions
# 1. Which filter do we return to the user ? filter0, filter1, filter2 or filter3? Why?
# Answer 1. We return filter3, because it is the best filter as it was learned from all the data available.
# 2. What is the estimate of the generalization error of the filter returned to the user? err0, err1, err2 or err3? Why?
# Answer 2. The estimated generalization error of filter3 is given by err2. Note that err0 is
# optimistic (overfitting) since the validation data was used to select the model. So, err0
# should not be used as estimate of the generalization error. On the other hand, err1 is a
# valid estimate since it is based on previously unseen data. However, it is pessimistic
# because the filter is trained on just tr. Finally, err3 is again optimistic because the
# filter's training data includes te. So, it shouldn't be used. In summary, a correct estimate
# is based on previously unseen data, and we should use the rest of the data available to train
# the filter (because only then the filter being evaluated will resemble the filter we return
# to the user).
# 3. Implementation of SVM predictions.
rbfkernel <- rbfdot(sigma = 0.05)
sv<-alphaindex(filter3)[[1]]
co<-coef(filter3)[[1]]
inte<- - b(filter3)
k<-NULL
for(i in 1:10){ # We produce predictions for just the first 10 points in the dataset.
k2<-NULL
for(j in 1:length(sv)){
k2<-c(k2,co[j]*rbfkernel(unlist(spam[sv[j],-58]),unlist(spam[i,-58])))
}
k<-c(k,sum(k2)+inte)
}
k
predict(filter3,spam[1:10,-58], type = "decision")
predict(filter3,spam[1:10,-58])
setwd("~/Desktop/Masters - LIU/Course/Sem 2/Period 2/TDDE07 Bayesian Learning/Labs/baysian-learning/Lab1")
library(manipulate)
sucess <- 13
n_trails <- 50
failures <- 37
alpha <- 5
beta <- 5
# Task 1. a)
# Function to calculate and plot mean of posterior Distribution Random Draws
PosteriorDraws_MeanPlot <- function(alpha, beta, n, sucess_proportions){
mean_posterior <-  list()
for(i in 1:n){
posterior_draws = rbeta(i, alpha+sucess, beta+failures)
mean_posterior[i] <- mean(posterior_draws)
}
print(mean_posterior[n])
plot(c(1:n), mean_posterior, type = 'l', lwd = 1, col = "blue", xlab = "number of Random Draws",
ylab = 'mean of Draws', main = 'Mean plot Posteriors')
}
# Function to calculate and plot SD of posterior Distribution Random Draws
PosteriorDraws_SDPlot <- function(alpha, beta, n, sucess_proportions){
sd_posterior <- list()
for(i in 1:n){
posterior_draws = rbeta(i, alpha+sucess, beta+failures)
sd_posterior[i] <- sd(posterior_draws)
}
print(sd_posterior[n])
plot(c(1:n), sd_posterior, type = 'l', lwd = 1, col = "red", xlab = "number of Random Draws",  ylab = 'SD of Draws', main = 'SD plot Posteriors')
}
#manipulate(
#  PosteriorDraws_MeanPlot(alpha, beta, n, sucess/n_trails),
#  n = slider(1, 100000, step=100, initial = 10000, label = "Number of trails(Random Draws)")
#)
PosteriorDraws_MeanPlot(alpha, beta, 1000, sucess/n_trails)
# True mean of beta distribution -> alpha/(alpha+beta), so true mean of posterior
true_mean = (alpha+sucess)/((alpha+sucess)+(beta+failures))
legend("bottomright", inset=.02, c("Mean posterior Draws", "Theoretical Mean"), fill=c("blue", "green"))
abline(h=true_mean, col = "green", type="l", lty=2)
print(true_mean)
#manipulate(
#  PosteriorDraws_SDPlot(alpha, beta, n, sucess/n_trails),
#  n = slider(1, 100000, step=100, initial = 10000, label = "Number of trails(Random Draws)")
#)
PosteriorDraws_SDPlot(alpha, beta, 1000, sucess/n_trails)
# True variance of beta distribution -> (alpha*beta)/((alpha+beta+1) * (alpha + beta)^2)
true_sd = sqrt(((alpha+sucess)*(beta+failures))/((alpha+sucess+beta+failures+1) * (alpha + sucess + beta + failures)^2))
print(true_sd)
legend("bottomright", inset=.02, c("Standard Deviation posterior Draws", "Theoretical Standard Deviation"), fill=c("red", "green"))
abline(h=true_sd, col = "green", type="l", lty=2)
# Task 1. b)
posterior_draws <-  rbeta(10000, alpha+sucess, beta+failures)              # taking 10000 random draws from the posterior beta distribution
count_density_less <- length(posterior_draws[posterior_draws<0.3])         # counting number of draws with value less than 0.3
posterior_probablity_less <- count_density_less/10000                    # calculating probability of draws with value < 0.3
posterior_probablity_less_exact <- pbeta(0.3, alpha+sucess, beta+failures) # using pbeta to find the same probability using cumulative frequency of beta distribution
posterior_probablity_less_exact
posterior_probablity_less
# Task 1. c)
posterior_draws <-  rbeta(10000, alpha+sucess, beta+failures)              # taking 10000 random draws from the posterior beta distribution
posterior_logodds <- log(posterior_draws/(1-posterior_draws))              # converting to log odds using simulated draws from beta posterior
hist(posterior_logodds, freq = FALSE)                                                    # plotting posterior distribution of log-odds
lines(density(posterior_logodds))                                           # plotting posterior distribution of log-odds
y = c(33,24,48,32,55,74,23,76, 17)
mu = 3.5
n = 9
# 2.a)
tau_sq = sum((log(y)-mu)^2)/n
X_draws = rchisq(1E5,n-1)
sigma_sq = (n-1)*tau_sq / X_draws   # Simulating draws from Inv−χ2(n,τ) distribution
# Theoretical density function for Inv−χ2(n,τ) distribution
inversechisq <- function(x, v, s){
density = (((v/2)^(v/2))/gamma(v/2)) * (s^v) * (x^(-(v/2+1))) * (exp((-v*s^2)/(2*x)))
return (density)
}
# Plotting posterior draws from sampling
plot(density(sigma_sq), lwd=3, main="Simulated and theoretical posteriors for sigma^2")
# Plotting posterior distribution using therotical density function
lines(seq(0.1,15,by=0.1), inversechisq(seq(0.1,15,by=0.1), n, sqrt(tau_sq)), type = "l", col =  "red", lty=2, lwd=3)
legend("topright", inset=.02, c("Simulated Posterior Draws", "Theoretical Posterior Density"), fill=c("black", "red"))
# 2.b)
# Calculating and plotting Gini coefficient from posterior Draws
distribution_gini = 2 * pnorm(sqrt(sigma_sq/2)) - 1
plot(1:length(distribution_gini), sort(distribution_gini), type = "l")
plot(density(distribution_gini), main = "Posterior Distribution Gini Coefficent")
# 2.c)
num = length(distribution_gini)
sorted = sort(distribution_gini)
# Finding cumulative sum of densities
c_sum = cumsum(sorted)
total = c_sum[length(c_sum)]
# Normalizing
c_sum_norm <- c_sum / total
lower <- 0
upper <- 0
N = length(c_sum)
for (i in 1:as.integer(N/2)){
# Finding lower bound of 95% equal tail credible interval as the point which contains 2.5% of the distribution
if (c_sum_norm[i] < 0.025){
lower <- i
}
# Finding upper bound of 95% equal tail credible interval as the point which contains 97.5% of the distribution
if (c_sum_norm[N-i] > 0.975){
upper <- N-i
}
}
# Plotting Confidence Intervals
plot(density(distribution_gini), main = "95% Equal Trail Confidence Intervel - Posterior Distribution Gini Coefficent")
abline(v = sorted[lower], col = "green")
abline(v = sorted[upper], col = "green")
legend("topright", inset=.02, c("Confidence Intervel"), fill=c("green"))
# Checking if found confidence interval is correct
sum(sorted[lower:upper]/sum(sorted))
sum(sorted[1:lower]/sum(sorted))
sum(sorted[upper:N]/sum(sorted))
# 2.d)
# Finding density of gini distribution
dense = density(sorted)
# storing coordinate and desnity values in a dataframe
df = data.frame(x=dense$x,y=dense$y)
# sorting dataframe based on density values
df = df[order(df$y),]
# Finding cumulative sum of sorted density values
df$cumsum = cumsum(df$y)
# finding all points at which the cumulative sum is > 5% if the total density ( 95% Highest density posterior interval)
df = df[df$cumsum>.05 * sum(df$y),c("x","y","cumsum")]
head(df)
# verifying if we have found the correct interval
sum(df$y) / sum(dense$y)
# plotting the HDPI
points(df,col="red")
legend("topright", inset=.01, c("95% Equal Tail Confidence Intervel", "Highest Density Posterior Intervel"), fill=c("green", "red"))
#black lines indicate equal tail credible interval 95%
#red dots indicate 95% highest posterior density
#3.a
# Function for the posterior distribution with n data points
p_cond <- function(y,mu,k){
n = length(y)
res = (1/(2*pi*besselI(k,0))^n) * exp(sum(k*cos(y-mu))-k)
return(res)
}
y= c(1.83, 2.02, 2.33, -2.79, 2.07, 2.02, -2.44, 2.14, 2.54, 2.23)
# Simulating 15000 posterior distribution over a k grid of 0-10
N=1500
res = c()
used_k = c()
for (k in seq(0,10*N)){
res = c(res,p_cond(y,2.51,k/N))
used_k = c(used_k,k/N)
}
# Normalizing density values by dividing by total sum of densities
res_norm = res/sum(res)
# Plotting k values against densities
plot(used_k,res_norm, main = "Posterior Distribution of k Values")
# The mode of a distribution with a discrete random variable is the value of the term that occurs the most often
# i.e the value of k with the highest probability density
used_k[res==max(res)]
abline(v=used_k[res==max(res)], col = "red", type="l", lty=2)
legend("topright", inset=.01, c("Posterior mode of k"), fill=c("red"), )
#dense = density(res)
library(scales)
data = readRDS("Precipitation.rds")
# 3.1.a)
y = log(data)
n = length(y)
y_mean = mean(y)
# initializing hyper parameters
mu_0 = mean(y)
t_sq_0 = var(y)
sigma_sq_0 = 1.8
v0 = 100
# Plotting prior to check if it is similar to the data given
prior_sigmasq = c()
N_Draws = 2000
prior_mu = rnorm(N_Draws,mu_0,t_sq_0)
for(i in 1:N_Draws){
X = rchisq(1,v0)
prior_sigmasq = c(prior_sigmasq, v0*sigma_sq_0 / X)
}
# Drawing form the simulated prior mean and sigma^2
prior_datapoints = c()
for(i in 1:N_Draws){
prior_datapoints = c(prior_datapoints, rnorm(1, prior_mu[i],prior_sigmasq[i]))
}
# Plotting histogram of data points
hist(prior_datapoints)
mean(prior_datapoints)
var(prior_datapoints)
# Initial Setting
N_Draws = n
mu = rnorm(1,mu_0,t_sq_0)
X = rchisq(1,v0)
sigmasq = v0*sigma_sq_0 / X
Gibbs_posterior_mu = c()
Gibbs_posterior_sigma = c()
# Posterior form Gibbs Sampling
for( i in 1:N_Draws){
w = (n/sigmasq) / ( (n/sigmasq) + (1 / t_sq_0))
mu_n = w*y_mean + (1-w)*mu_0
t_sq_n = 1/((n/sigmasq) + (1/t_sq_0))
mu = rnorm(1,mu_n,t_sq_n)
Gibbs_posterior_mu = c(Gibbs_posterior_mu, mu)
v_n = v0 + n
X = rchisq(1,v_n)
thing = (v0*sigma_sq_0 + sum((mu - y)^2))/(n + v0)
sigmasq = v_n*thing/X
Gibbs_posterior_sigma = c(Gibbs_posterior_sigma, sigmasq)
}
acf_mu = acf(Gibbs_posterior_mu)
acf_sigma = acf(Gibbs_posterior_sigma)
# Calculating Inefficiency Factors for mu and sigma^2
if_mu = 1+2*sum(acf_mu$acf[-1])
if_sigma = 1+2*sum(acf_sigma$acf[-1])
hist(Gibbs_posterior_mu)
hist(Gibbs_posterior_sigma)
plot(Gibbs_posterior_mu,type="l", xlab = "iteration", ylab = "mu", main="Sampled Markov chain for Mu")
plot(Gibbs_posterior_sigma,type="l", xlab = "iteration", ylab = "sigma", main="Sampled Markov chain for sigma")
# Direct Draws Done for verification
mu_0 = mean(y)
k0 = 1
sigma_sq_0 = 1.8
v0 = 100
# Draws from Piror
nDraws = 2000
X = rchisq(nDraws,v0)
prior_direct_sigmasq = v0*sigma_sq_0 / X
prior_direct_mu = c()
for(i in 1:nDraws){
prior_direct_mu = c(prior_direct_mu, rnorm(1,mu_0,prior_direct_sigmasq[i]/k0))
}
hist(mu)
hist(prior_direct_sigmasq)
# Posterior from Direct Sampling
nDraws = 500
mu_n = ((k0/(k0+n))*mu_0) + ((n/(k0+n))*y_mean)
kn = k0 + n
vn = v0 + n
s_sq = sum((y-y_mean)^2)/(n-1)
sigmasq_n = (v0*sigma_sq_0 + (n-1)*s_sq + (((k0*n)/(k0+n))*(y_mean - mu_0)^2))/vn
X = rchisq(nDraws,vn)
direct_sigmasq = vn*sigmasq_n / X
direct_mu = c()
for(i in 1:nDraws){
direct_mu = c(direct_mu, rnorm(1,mu_n,direct_sigmasq[i]/kn))
}
hist(direct_mu)
hist(direct_sigmasq)
plot(direct_mu,type="l")
plot(direct_sigmasq,type="l")
# lab 3.1.b
hist(data)
draws_posterior = c()
for(i in 1:n){
draws_posterior = c(draws_posterior, rnorm(1,Gibbs_posterior_mu[i], Gibbs_posterior_sigma[i]))
}
hist(exp(draws_posterior), breaks = 100, xlim=range(0,80))
library(mvtnorm)
data <- read.table("eBayNumberOfBidderData.dat", sep = "" , header = T , na.strings ="", stringsAsFactors= F)
data = as.data.frame(data)
col_names = colnames(data)
# lab 3.2.1
model <- glm(nBids ~ . , data[,-2], family = poisson(link = "log"))
model
summary(model)
# lab 3.2.2
Y = data$nBids
X = as.matrix(data[,-1])
n = dim(data)[1]
mu = rep(0,9)
Sigma = 100*solve(t(X)%*%X)
beta_prior = rmvnorm(1,mean=mu,sigma=Sigma)
dim(beta_prior)
LogPostPoisson <- function(betas,Y,X,mu,Sigma){
linPred <- betas%*%t(X);
# finding the log likelihood
logLik <- sum(Y*linPred - exp(linPred) - log(factorial(Y)));
#print(logLik)
# finding the log logPrior using the parameters given
logPrior <- dmvnorm(betas, mu, Sigma, log=TRUE);
#print(logPrior)
# returning the log posterior as the sum of loglikihood and logPrior
return(logLik + logPrior)
}
OptimRes <- optim(beta_prior,LogPostPoisson,gr=NULL,Y,X,mu,Sigma,method=c("BFGS"),control=list(fnscale=-1),hessian=TRUE)
# Getting the hessian matrix from the results returned from the optim function
beta_mode_hessian = OptimRes$hessian
# Getting the mode β values as the parameters returned from the optim function
beta_mode = OptimRes$par
colnames(beta_mode) = col_names[-1]
# Getting the J(beta_mode) as the negative of the hessian returned
J = -beta_mode_hessian
# Calculating inverse of J
J_inverse = solve(-beta_mode_hessian)
posterior_beta_draw = rmvnorm(1,beta_mode, J_inverse)
# lab 3.2.3
num_random_walk = 1E4
# Function for metropolis random walk
MetropolisRandomWalk <- function(beta_initial, c, J_inverse, log_density_function, mu, Sigma, Y, X){
metropolis_betas = matrix(data = NA, nrow = num_random_walk, ncol = dim(X)[2])
# Initializing previous betas as the initial value
prev_beta = beta_initial
for(i in 1:num_random_walk){
# Sampling betas from the multivariate normal distribution
sample = rmvnorm(1, mean = prev_beta, sigma = c * J_inverse)
# Calculating the log of the density values for the sampled betas
log_density_sample = log_density_function(sample, Y, X, mu, Sigma)
# Calculating the log of the density values for the previous samples of accepted betas
log_density_prev_sample = log_density_function(prev_beta, Y, X, mu, Sigma)
# Calculating alpha values
alpha = min(1, exp((log_density_sample - log_density_prev_sample)))
# Generating a random number between 0 and 1
rand = runif(n=1, min = 0, max = 1)
# Accepting sample based on the probability alpha
if(rand<alpha){
metropolis_betas[i,] = sample
prev_beta = sample
}
else{
metropolis_betas[i,] = prev_beta
}
}
return(metropolis_betas)
}
# Sampling betas based using Metropolis Random Walk
metropolis_posterior_betas = MetropolisRandomWalk(posterior_beta_draw, .6, J_inverse, LogPostPoisson, mu, Sigma, Y, X)
colnames(metropolis_posterior_betas) = col_names[-1]
# plotting Sampled value to access the convergence of betas
par(mfrow=c(3,3))
for(i in 1:dim(X)[2]){
plot(metropolis_posterior_betas[,i], type = "l", ylim = c(0,2*model$coefficients[i]))
abline(h=model$coefficients[i], col="red")
}
# lab 3.2.4
par(mfrow=c(1,1))
# Creation data for test case
sample = matrix(c(1, 1, 0, 1, 0, 1,0,1.2,0.8))
# Getting predictions for the test data based on sampled betas as the mean of the Poisson Distribution is parameters passed it
# The plot of the parameters passed to it (exp(betas*X)) would be good plot of the predictive distribution
prediction = exp(metropolis_posterior_betas %*% sample)
#print(exp(model$coefficients%*%sample))
hist(prediction, main = "Predictive distribution", sub =paste("glm predicts: ",exp(model$coefficients%*%sample)))
# Calculating probability of no bidders
prob_no_bids = mean(rpois(num_random_walk,prediction)==0)
mu = 13
sigma_sq = 3
t_max = 300
fi = 1
# lab 3.3.1
# Function to simulate Auto Regressive Function
AR_Process <- function(mu, FI, sigma_sq)
{
path = matrix(data = NA, nrow = length(FI), ncol = t_max)
i = 1
for (fi in FI){
path[i,1] = mu
for (t in 2:t_max){
path[i,t] = mu + fi*(path[i,t-1]-mu) + rnorm(1,0,sigma_sq)
}
i = i+1
}
return(path)
}
# Simulating Draws from the AR1 process with phi value -0.8, 0, 0.8
FI = seq(-.8,.8,.8)
path = AR_Process(mu, FI, sigma_sq)
par(mfrow=c(3,2))
for (fi in 1:length(FI)){
plot(path[fi,],main=paste("phi = ",FI[fi]),type="line",col="blue")
acf(path[fi,],main=paste("ACF phi = ",FI[fi]))
}
# lab 3.3.2
# Creatind Data to pass to stan model
fi_1 = .2
fi_2 = .95
FI = c(fi_1, fi_2)
path = AR_Process(mu, FI, sigma_sq)
par(mfrow=c(2,2))
for (fi in 1:length(FI)){
plot(path[fi,],main=paste("phi = ",FI[fi]),type="line",col="blue")
acf(path[fi,],main=paste("ACF phi = ",FI[fi]))
}
library(rstan)
# Creating Stan Model to simulate the two AR(1)-processes
StanModel = '
data {
matrix[300,2] M; // synthetic data
}
parameters {
real mu;
real<lower=0> sigma2;
real phi_1;
real phi_2;
}
model {
mu ~ uniform(-100,200); // Normal with mean 0, st.dev. 100
sigma2 ~ uniform(0,100); // Scaled-inv-chi2 with nu 1,sigma 2
phi_1 ~ uniform(-10,10);
phi_2 ~ uniform(-10,10);
for(i in 2:300){
M[i,1] ~ normal(mu + phi_1*(M[i-1,1]-mu),sigma2);
M[i,2] ~ normal(mu + phi_2*(M[i-1,2]-mu),sigma2);
}
}'
data <- list(M=t(path))
warmup <- 10
niter <- 2000
fit <- stan(model_code=StanModel,data=data, warmup=warmup,iter=niter,chains=4)
# Print the fitted model
print(fit,digits_summary=3)
# Extract posterior samples
postDraws <- extract(fit)
# Do traceplots of the first chain
par(mfrow = c(1,1))
# Do automatic traceplots of all chains
traceplot(fit)
# Plotting joint posterior for mu and phi1
plot(postDraws$mu[1:(niter-warmup)], postDraws$phi_1[1:(niter-warmup)], xlab = "mu", ylab = "phi_1", main = "Joint Posterior of mu and phi 1")
# Plotting joint posterior for mu and phi2
plot(postDraws$mu[1:(niter-warmup)], postDraws$phi_2[1:(niter-warmup)], xlab = "mu", ylab = "phi_1", main = "Joint Posterior of mu and phi 2")
# Bivariate posterior plots
pairs(fit)
